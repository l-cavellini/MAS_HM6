{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Reinforcement Learning\n",
    "Importing the libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating the classes and defining the method to print the output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class State():\n",
    "    def __init__(self, type: str, pos: tuple) -> None:\n",
    "        self.pos = pos # (x,y)\n",
    "        self.type = type\n",
    "        self.terminal = False\n",
    "        self.reward = None \n",
    "        self.actions = ['left', 'right', 'up', 'down'] \n",
    "        self.Q = {'left': 0, 'right': 0, 'up': 0, 'down': 0} \n",
    "        self.model = {'left': [], 'right': [], 'up': [], 'down': []} \n",
    "\n",
    "    def init_state(self):\n",
    "        # starting cell  \n",
    "        if self.type == 'start':\n",
    "            self.reward = -10000\n",
    "        # normal cell\n",
    "        if self.type == 'cell':\n",
    "            self.reward = -1\n",
    "        # cliff cell -> gotta return back starting state \n",
    "        if self.type == 'cliff': \n",
    "            self.reward = -100\n",
    "            self.terminal = True\n",
    "        # goal cell\n",
    "        if self.type == 'goal':\n",
    "            self.reward = 20\n",
    "            self.Q['left'] = 1\n",
    "            self.terminal = True\n",
    "        # snake cell \n",
    "        if self.type == 'snake':\n",
    "            self.reward = -100\n",
    "            #self.terminal = True does it terminate the episode?\n",
    "\n",
    "    def policy(self, epsilon=0.1) -> str:\n",
    "\n",
    "        # compute Q(s,a) value for all actions & get max \n",
    "        Q = [self.Q[a] for a in self.actions]\n",
    "        max_qa = max(Q)\n",
    "        max_qa_id = Q.index(max_qa)\n",
    "        # selects action based on e-greedy \n",
    "        if random.random() < 1-epsilon:\n",
    "            return self.actions[max_qa_id]\n",
    "        else: \n",
    "            return random.choice(self.actions)\n",
    "\n",
    "\n",
    "class CliffWorld:\n",
    "    def __init__(self, size: tuple = (4,21)) -> None:\n",
    "        # parameters\n",
    "        self.lr = 0.4\n",
    "        self.discount = 0.9 #put sizes\n",
    "        self.rows = size[0]\n",
    "        self.cols = size[1] \n",
    "        self.grid = [[None for j in range(self.cols)] for i in range(self.rows)]\n",
    "        self.actions = ['left', 'right', 'up', 'down']\n",
    "        self._init_world()\n",
    "        \n",
    "    def add_state(self, type: str, pos: tuple):\n",
    "        # adds state to grid according to cell type \n",
    "        state = State(type, pos)\n",
    "        state.init_state()\n",
    "        x, y = pos[0], pos[1]\n",
    "        self.grid[x][y] = state\n",
    "    \n",
    "    def _init_world(self): \n",
    "        # add cliff\n",
    "        for i in range(1,20):\n",
    "            self.add_state('cliff', (3,i))\n",
    "        # add goal and snake\n",
    "        self.add_state('start', (3,0))\n",
    "        self.add_state('goal', (3,20))\n",
    "        #self.add_state('snake', (0,11))\n",
    "        # add cell states \n",
    "        for i, row in enumerate(self.grid):\n",
    "            for j, val in enumerate(row): \n",
    "                if val is None:\n",
    "                    self.add_state('cell', (i,j))\n",
    "    \n",
    "\n",
    "    def get_state(self, pos: tuple) -> State:\n",
    "        x, y = pos[0], pos[1]\n",
    "        return self.grid[x][y]\n",
    "    \n",
    "    def move_position(self, current_pos: tuple, action: str) -> tuple:\n",
    "        # state = self.get_state(current_pos)\n",
    "        x, y = current_pos[0], current_pos[1]\n",
    "        # possible to pick any action, but if pick edges -> stay in spot & if pick wall -> stay in spot \n",
    "        if action == 'left':\n",
    "            next_pos = (x, y-1)\n",
    "        elif action == 'right':\n",
    "            next_pos = (x, y+1)\n",
    "        elif action == 'up':\n",
    "            next_pos = (x-1, y)\n",
    "        elif action == 'down':\n",
    "            next_pos = (x+1, y)\n",
    "        # check for boundary/wall conditions \n",
    "        if next_pos[0] < 0 or next_pos[0] > 3 or next_pos[1] < 0 or next_pos[1] > 20:\n",
    "            return current_pos\n",
    "        if self.get_state(next_pos).type == 'cliff':\n",
    "            self.get_state((3,0)).terminal = True\n",
    "            next_pos = (3,0)\n",
    "            \n",
    "        return next_pos\n",
    "    \n",
    "def show_policy(cw: CliffWorld):\n",
    "    temp = np.zeros((cw.rows,cw.cols), str)\n",
    "    for i in range(cw.rows):\n",
    "        for j in range(cw.cols):\n",
    "            cell = cw.get_state((i,j))\n",
    "            max_item = max(cell.Q.items(), key=lambda x: x[1])\n",
    "            if max_item[1] == 0:\n",
    "                temp[i][j] ='0'\n",
    "            elif max_item[1] == 1:\n",
    "                temp[i][j] ='G'\n",
    "            elif max_item[0] == 'right':\n",
    "                temp[i][j] ='>'\n",
    "            elif max_item[0] == 'left':\n",
    "                temp[i][j] ='<'\n",
    "            elif max_item[0] == 'down':\n",
    "                temp[i][j] ='v'\n",
    "            elif max_item[0] == 'up':\n",
    "                temp[i][j] ='^'\n",
    "       \n",
    "    for i in range(cw.rows):\n",
    "            print('-------------------------------------------------------------------------------------')\n",
    "            out = '| '\n",
    "            for j in range(cw.cols):\n",
    "                out += temp[i][j] + ' | '\n",
    "            print(out)\n",
    "    print('-------------------------------------------------------------------------------------')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Defining the methods for the SARSA algorithm and Q-Learning "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SARSA(grid_world, episodes=10000): \n",
    "    action_counter_total = []\n",
    "    reward_counter_total = []\n",
    "    # episodes  \n",
    "    for _ in range(episodes):\n",
    "        # initialize agent position at the start\n",
    "        agent_pos = (3,0)\n",
    "        \n",
    "        # get state for the agent \n",
    "        s = grid_world.get_state(agent_pos)\n",
    "\n",
    "        # -- 1 episode -- \n",
    "        action_counter = 0\n",
    "        reward_counter = 0\n",
    "        s.terminal = False\n",
    "        while s.terminal == False:\n",
    "          # get the policy for that s -> gives a \n",
    "            a = s.policy(0.05)\n",
    "            # take an action -> arrive in s'\n",
    "            next_pos = grid_world.move_position(agent_pos, a)\n",
    "            s_prime = grid_world.get_state(next_pos)\n",
    "            # get transition reward \n",
    "            r = s_prime.reward\n",
    "            # get policy for s' -> gives a'\n",
    "            a_prime = s_prime.policy(0.05)\n",
    "            # Q(s,a) update \n",
    "            new_q_sa = s.Q[a] + grid_world.lr  * (r + (grid_world.discount * s_prime.Q[a_prime]) - s.Q[a])\n",
    "            s.Q[a] = new_q_sa\n",
    "            # transitions for next iteration \n",
    "            s, a, agent_pos = s_prime, a_prime, next_pos\n",
    "            # counters \n",
    "            action_counter += 1\n",
    "            reward_counter += r\n",
    "\n",
    "        action_counter_total.append(action_counter)\n",
    "        reward_counter_total.append(reward_counter)\n",
    "\n",
    "    return grid_world, action_counter_total, reward_counter_total\n",
    "\n",
    "\n",
    "def Q_learning(grid_world, episodes=10000):\n",
    "    action_counter_total = []\n",
    "    reward_counter_total = []\n",
    "    # -- episodes -- \n",
    "    for _ in range(episodes):\n",
    "        # initialize agent position \n",
    "        agent_pos = (3,0)\n",
    " \n",
    "        # get state for the agent \n",
    "        s = grid_world.get_state(agent_pos)\n",
    "        s.terminal = False\n",
    "        # -- 1 episode -- \n",
    "        action_counter = 0\n",
    "        reward_counter = 0\n",
    "        while s.terminal == False:\n",
    "            # get the policy for that s -> gives a \n",
    "            a = s.policy()\n",
    "            # take an action -> arrive in s'\n",
    "            next_pos = grid_world.move_position(agent_pos, a)\n",
    "            s_prime = grid_world.get_state(next_pos)\n",
    "            # get transition reward \n",
    "            r = s_prime.reward\n",
    "            # get max Q(s, a')\n",
    "            max_a_prime = max(s.Q.items(), key=lambda x: x[1])[0]\n",
    "            # Q(s,a) update \n",
    "            new_q_sa = s.Q[a] + grid_world.lr  * (r + (grid_world.discount * s_prime.Q[max_a_prime]) - s.Q[a])\n",
    "            s.Q[a] = new_q_sa\n",
    "            # transitions for next iteration \n",
    "            s, agent_pos = s_prime, next_pos\n",
    "            # counters\n",
    "            action_counter += 1\n",
    "            reward_counter += r\n",
    "        action_counter_total.append(action_counter)\n",
    "        reward_counter_total.append(reward_counter)\n",
    "\n",
    "    return grid_world, action_counter_total, reward_counter_total"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment \n",
    "1. SARSA with epsilon = 0.1, without the snake "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "| > | v | < | < | < | v | < | < | < | > | > | > | > | > | > | > | > | > | > | > | v | \n",
      "-------------------------------------------------------------------------------------\n",
      "| > | ^ | ^ | ^ | > | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | v | \n",
      "-------------------------------------------------------------------------------------\n",
      "| ^ | ^ | ^ | > | > | ^ | ^ | ^ | ^ | < | ^ | ^ | ^ | ^ | ^ | ^ | > | ^ | < | > | v | \n",
      "-------------------------------------------------------------------------------------\n",
      "| ^ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | G | \n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cw = CliffWorld()\n",
    "cw, _, __ = SARSA(cw) # epsilon = 0.1\n",
    "show_policy(cw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Q-Learning \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "| > | > | > | > | > | > | > | v | > | > | > | > | > | > | v | v | > | v | v | > | v | \n",
      "-------------------------------------------------------------------------------------\n",
      "| > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | v | \n",
      "-------------------------------------------------------------------------------------\n",
      "| > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | > | v | \n",
      "-------------------------------------------------------------------------------------\n",
      "| ^ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | G | \n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cw = CliffWorld()\n",
    "cw, _, __ = Q_learning(cw) \n",
    "show_policy(cw)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. SARSA with epsilon = 0.05"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------------------------------------------------------\n",
      "| > | > | > | > | > | > | > | ^ | > | > | > | > | > | > | > | > | > | > | > | > | v | \n",
      "-------------------------------------------------------------------------------------\n",
      "| > | > | ^ | ^ | ^ | ^ | ^ | ^ | > | ^ | ^ | ^ | > | ^ | ^ | > | > | > | ^ | ^ | v | \n",
      "-------------------------------------------------------------------------------------\n",
      "| ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | > | > | ^ | ^ | ^ | ^ | ^ | ^ | ^ | ^ | < | v | \n",
      "-------------------------------------------------------------------------------------\n",
      "| ^ | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | 0 | G | \n",
      "-------------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "cw = CliffWorld()\n",
    "cw, _, __ = SARSA(cw) # epsilon = 0.0.5\n",
    "show_policy(cw)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
